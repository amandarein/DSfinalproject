---
title: "Final Project -- KNN Analysis"
author: "Belen Gomez Grimaldi, Amanda Rein, Kay Mattern"
date: "4/20/2021"
output:
  html_document:
    toc: TRUE
    theme: journal
    toc_float: TRUE
editor_options:
  chunk_output_type: console
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load-packages, include=FALSE}
install.packages("naniar")
install.packages("randomForest")
install.packages("rio")
library(randomForest)
library(rio)
library(dplyr)
library(magrittr)
library(knitr)
library(naniar)
library(ggplot2)
library(plotly)
library(caret)
```

# Mission
Our goal is to create a system that can properly detect whether a Twitter account is a "bot", meaning that it is not controlled by a human but instead by some algorithm or automated program. Twitter and other social media companies struggle to detect these "bot" accounts but seek to do so in order to delete them, as it is not the intent of a social media or communication platform to include bots that are falsifying themselves as real people! We found a data set that includes many different metrics about Twitter accounts, such as number of followers, number of friends, language of account, and number of favorites. Each of these accounts is labeled as a bot or not, too, which gives us the opportunity to use this data to train a model to detect which Twitter accounts are bots! We see this as not only a great opportunity to help Twitter remove their bot accounts; it is also good practice for us in training and evaluating models via the techniques we learned this past semester.

# Data Cleaning
Data cleaning is always an important first step to building predictive models, as data sets are often riddled with empty cells, malformed cells, and even highly correlated rows that do not both need to be present in the set. 

## Find out which columns have many missing values so that they can be discarded
```{r, message=FALSE, echo = FALSE, warning=FALSE}
twitter_data = read.csv("twitterBots.csv")
gg_miss_var(twitter_data)
twitter_data <- twitter_data[complete.cases(twitter_data), ]

twitter_data <- twitter_data[,-c(1,2,3,4,5,6,10,15,19)]
```

We remove the rows with incomplete cases because we see with this gg_miss_var plot that there are nearly 600 cases that are missing "bot" values. Since we are created a system that predicts whether a Twitter account is a bot, we need each data point for training/testing to have a value for this. Additionally, we remove id, id_str, screen_name, location, description, url, created_at, status, and name because these are text columns that are unique for each data point and thus unnecessary to include in the training.

```{r, message=FALSE, echo = FALSE, warning=FALSE}
head(twitter_data)
```

This is now what the twitter_data dataframe looks like. We have 10 variables on which to form predictions, then the 'bot' variable that is 1 when a Twitter account is a bot and 0 when it is not. 

We now check the split of data to find the base rate.
```{r, message=FALSE, echo = FALSE, warning=FALSE}
split <- table(twitter_data$bot)[2] / sum(table(twitter_data$bot))
split
```

The split is 47.2%, meaning that roughly 47% of our data is for bots ("bot" = 1). This indicates a well-balanced data set, which is ideal for building a model.

Now, we have to set the remaining variables as factors and then all (besides "bot") as numeric for analysis. We also scale all but "bot" for the knn training.
```{r, message=FALSE, echo = FALSE, warning=FALSE}
twitter_data[,c(5, 7, 8, 9, 10, 11)] <- lapply(twitter_data[,c(5, 7, 8, 9, 10, 11)], as.factor)
twitter_data[,-11] <- lapply(twitter_data[,-11], as.numeric)

# Scale the data
twitter_data[, -c(11)] <- lapply(twitter_data[, -c(11)],function(x) scale(x))
```

Now, we check the correlation plot to see what we might want to remove from the dataframe. If two variables are highly correlated, then we don't need both of them for training our system.
```{r, message=FALSE, echo = FALSE, warning=FALSE}
# Find correlations in the data 
stat_correlations <- cor(twitter_data[,-11])
stat_correlations
twitter_data <- subset(twitter_data, select = -listed_count)
```

The variables listed_count and followers_count have a correlation coefficient of 0.810702064. The rest of the pairs of variables have correlation coefficients below 0.5, so we will remove one of the two from that highly correlated pair, listed_count (though we could have removed followers_count instead). 

# Training the Model
## Splitting into Training and Testing Sets
Then, we split into train and test sets so that 80% of our data would be used for training and 20% for testing. 
```{r, message=FALSE, echo = FALSE, warning=FALSE}
set.seed(1982)
bot_data_train_rows = sample(1:nrow(twitter_data),
                              round(0.8 * nrow(twitter_data), 0),
                              replace = FALSE)
# Check to make sure we have 80% of the rows
percent_or_rows = length(bot_data_train_rows) / nrow(twitter_data)
# Rows used in training set
bot_data_train = twitter_data[bot_data_train_rows, ]
# Rows not used in training set, aka the test set
bot_data_test = twitter_data[-bot_data_train_rows, ]
# Check the number of rows in each set.
nrow(bot_data_train)
nrow(bot_data_test)
```

We confirm here that our train set has 80% of our data. We see that bot_data_train (our training set) has 2238 elements while bot_data_test (our testing set) has 559 elements. This aligns with our goal for separating our data points with 80-20% train-test.

## Elbow Plot to find Best K Value for KNN
Now, we create an elbow plot to check which k value would maximize the accuracy of our knn model.
```{r, message=FALSE, echo = FALSE, warning=FALSE}
# Figure out which K to use
# install.packages("class") 
library(class)
chooseK = function(k, train_set, val_set, train_class, val_class){
  set.seed(1)
  class_knn = knn(train = train_set,
                  test = val_set,
                  cl = train_class,
                  k = k,
                  use.all = TRUE)
  conf_mat = table(class_knn, val_class)
  test <- conf_mat
  # Accuracy = (TP + TN) / (TP + TN + FP + FN)
  accu = sum(conf_mat[row(conf_mat) == col(conf_mat)]) / sum(conf_mat)
  cbind(k = k, accuracy = accu)
}
knn_diff_k_bot = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
                         function(x) chooseK(x,
                                             train_set =
                                               bot_data_train[, -c(10)],
                                             val_set = bot_data_test[, -c(10)],
                                             train_class = bot_data_train[, 10],
                                             val_class = bot_data_test[, 10]))
knn_diff_k_bot = tibble(k = knn_diff_k_bot[1,],
                             accuracy = knn_diff_k_bot[2,])
ggplot(knn_diff_k_bot,
       aes(x = k, y = accuracy)) +
  geom_line(color = "orange", size = 1.5) +
  geom_point(size = 3)
```

We see that 11 is our best k value for training our model because it gives us a model with the highest accuracy. 

## Run KNN with Best K Value (K = 11)
Now, we run KNN analysis with 11 nearest neighbors and analyze the accuracy of the model
```{r, message=FALSE, echo = FALSE, warning=FALSE}
bot_11NN <-  knn(train = bot_data_train[, -10],
               test = bot_data_test[, -10],
               cl = bot_data_train[, 10],
               k = 11,
               use.all = TRUE,
               prob = TRUE)

kNN_res = table(bot_11NN,
                bot_data_test$bot)
# View(kNN_res)
conf_matrix_initial <- kNN_res
# conf_matrix_initial
# install.packages("caret")
library(caret)
# install.packages("e1071")
library(e1071)
# install.packages("Rcpp")
library(Rcpp)
```


# Evaluate Model {.tabset}
## Confusion Matrix
```{r, message=FALSE, echo = FALSE, warning=FALSE}
conf_matrix <- confusionMatrix(as.factor(bot_11NN), as.factor(bot_data_test$bot), positive = "1", dnn=c("Prediction", "Actual"), mode = "sens_spec")
conf_matrix 
# conf_matrix$overall["Accuracy"]
# conf_matrix$overall["Kappa"]
# conf_matrix$byClass["Sensitivity"]
# conf_matrix$byClass["Specificity"]
conf_matrix$byClass["F1"]
```

We see from this confusion matrix that the accuracy is 86.23%, which is far above the base rate of 47%. This means that about 86% of the time, our system can correctly differentiate between a bot and a non-bot account.
The kappa is 0.7245, which is considered moderately strong for inter-rater reliability. 
The sensitivity (or proportion of true positives) is 0.85, which means there is an 85% chance of the system correctly identifying a bot.
The specificity (or proportion of true negatives) is 0.8746, which means that there is an 87.5% chance of detecting when something is not a bot.
The F1 is 0.86. This indicates a decently strong accuracy with our system. 
We also see in this confusion matrix that there are 35 cases of false positives (or 12.5% of non-bots are mislabeled) and 42 cases of false negatives (or 15% of bots are mislabeled). We have a  well-balanced dataset, with 279 cases of non-bots and 280 cases of bots, too.

## Log Loss
```{r, message=FALSE, echo = FALSE, warning=FALSE}
# install.packages("MLmetrics")
library(MLmetrics)
# str(as.numeric(attributes(bot_11NN)$prob))
# str(bot_data_test$bot)

# attributes(bot_11NN)$prob
bot_11NNDF <- as.data.frame(bot_11NN)

test <- ifelse(bot_11NNDF[,'bot_11NN'] == 1, attributes(bot_11NN)$prob,1-attributes(bot_11NN)$prob)
# test

LogLoss(test, as.numeric(bot_data_test$bot))
```

We have a LogLoss score of 2.78 for this model. LogLoss is a measurement of how close the prediction probability is to the actual value. As such, this score will penalize very confident incorrect guesses by the system. This score is higher than we'd like for our model, so this means that our model is confidently making some false predictions. This is something we will keep in mind!

## AUC
```{r, message=FALSE, echo = FALSE, warning=FALSE}
# install.packages("ROCR")
library(ROCR)
pred <- prediction(test, as.numeric(bot_data_test$bot))
# View(pred)
perf <- performance(pred,"tpr","fpr")
plot(perf, colorize=TRUE)
abline(a=0, b= 1)
perf_AUC <- performance(pred,"auc")
perf_AUC@y.values[[1]]
```

As you can see from this graph, our AUC is pretty good! There is significant area between our curve (the multicolored one) and the y = x line that represents random guessing. The AUC value is 0.93, which tells us that our model is pretty good at distinguishing between our two classes (whether or not an account is a bot).

# Adjusting Threshold for Misclassification Errors
We have slightly more false negatives than false positives. That being said, we feel that the priority for this system is to detect when an account is actually a bot account. Assuming that this system is used to flag/remove Twitter accounts that are bots, if we mislabel an account as a non-bot, then we may delete the account when it holds meaning for its human owner. So, we want to minimize false positives. This means that we should increase the threshold.
```{r, message=FALSE, echo = FALSE, warning=FALSE}
adjust_thres <- function(x, y, z) {
  #x=pred_probablities, y=threshold, z=test_outcome
  thres <- as.factor(ifelse(x > y, 1,0))
  confusionMatrix(thres, z, positive = "1", dnn=c("Prediction", "Actual"), mode = "everything")
}
bot_refactor <- as.data.frame(bot_11NN)
probs <- attributes(bot_11NN)$prob
bot_refactor <- cbind(bot_refactor, probs)
bot_refactor_1 <- mutate(bot_refactor, `1` = ifelse(bot_11NN == "1", probs, (1-probs)))
bot_refactor_1 <- mutate(bot_refactor_1, `0` = ifelse(bot_11NN == "0", probs, (1-probs)))

# thresholds <- lapply(0.2:0.5:0.05, as.factor, x=bot_refactor_1$`1`, z=as.factor(bot_data_test$bot))
# adjust_thres(bot_refactor_1$`1`, 0.9, as.factor(bot_data_test$bot))
```
* When we adjust the threshold to 0.6, we have an accuracy of 0.8479, a kappa of 0.6959, a sensitivity of 0.7964, and a specificity of 0.8996. We have 28 cases of false positives.

* When we adjust the threshold to 0.7, we have an accuracy of 0.8336, a kappa of 0.6674, a sensitivity of 0.7429, and a specificity of 0.9247. We have 21 cases of false positives.

* When we adjust the threshold to 0.8, we have an accuracy of 0.8032, a kappa of 0.6066, a sensitivity of 0.6679, and a specificity of 0.9391. We have 17 cases of false positives.

* When we adjust the threshold to 0.9, we have an accuracy of 0.7746, a kappa of 0.5495, a sensitivity of 0.5857, and a specificity of 0.9642 We have 10 cases of false positives. 

# Random Forest
```{r, message=FALSE, echo = FALSE, warning=FALSE}

mytry_tune <- function(x){
  xx <- dim(x)[2]-1
  sqrt(xx)
}

bot_mytry <- mytry_tune(twitter_data)

set.seed(1900)
bot_rf = randomForest(as.factor(bot)~.,
                            bot_data_train,
                            ntree = 1000,
                            mtry = bot_mytry,
                            replace = TRUE,
                            sampsize = 100,
                            nodesize = 5,
                            importance = TRUE,
                            proximity = TRUE,
                            norm.votes = TRUE,
                            do.trace = TRUE,
                            keep.forest = TRUE,
                            keep.inbag = TRUE)  

bot_rf
table(twitter_data$bot)
1321/2797

```

# Evaluate Model {.tabset}
## Confusion Matrix
```{r, message=FALSE, echo = FALSE, warning=FALSE}

bot_rf$confusion
bot_rf_acc = sum(bot_rf$confusion[row(bot_rf$confusion) == 
                                                col(bot_rf$confusion)]) / 
  sum(bot_rf$confusion)

# Accuracy
bot_rf_acc #.88 (pretty good)

```

## Variable Importance
```{r, message=FALSE, echo = FALSE, warning=FALSE}
# The "importance" argument provides a table that includes the importance
# of each variable to the accuracy of the classification.
# View(as.data.frame(importance(bot_rf, type = 2, scale = TRUE))) #type 1 is error on oob, 
                                                                      # type 2 is total decrease
# in node impurity as measured by the Gini index, look at the differences, stop by wine for example. 
                                                        # scale divides the measures by 
                                                        # their standard errors

as.data.frame(bot_rf$importance) #all the metrics together,not scaled

# Looks like friends_count is most important variable

```

## Error Rates
```{r, message=FALSE, echo = FALSE, warning=FALSE}

#View(as.data.frame(bot_rf$err.rate))

err.rate <- as.data.frame(bot_rf$err.rate) 

# for each tree

#View(err.rate)

```

## Error Visualization
```{r, message=FALSE, echo = FALSE, warning=FALSE}

bot_rf_error = data.frame(1:nrow(bot_rf$err.rate),
                                bot_rf$err.rate)

colnames(bot_rf_error) = c("Number of Trees", "Out of the Box",
                                 "Not bot", "Bot")

bot_rf_error$Diff <- bot_rf_error$Bot-bot_rf_error$`Not bot`

#View(bot_rf_error)

fig <- plot_ly(x=bot_rf_error$`Number of Trees`, y=bot_rf_error$Diff,name="Diff", type = 'scatter', mode = 'lines')
fig <- fig %>% add_trace(y=bot_rf_error$`Out of the Box`, name="OOB_Er")
fig <- fig %>% add_trace(y=bot_rf_error$`Not bot`, name="Not bot")
fig <- fig %>% add_trace(y=bot_rf_error$Bot, name="Bot")

fig
```

# Optimized Random Forest Model
```{r, message=FALSE, echo = FALSE, warning=FALSE}

# Depends what metric we want to optimize... we can decide later but for this one I'll focus on correctly identifying bots (positive class)

#View(bot_rf_error)

# Minimize bot and OOB --> we'll try 20 trees
set.seed(1900)
op_bot_rf = randomForest(as.factor(bot)~.,
                            bot_data_train,
                            ntree = 20,
                            mtry = bot_mytry,
                            replace = TRUE,
                            sampsize = 100,
                            nodesize = 5,
                            importance = TRUE,
                            proximity = TRUE,
                            norm.votes = TRUE,
                            do.trace = TRUE,
                            keep.forest = TRUE,
                            keep.inbag = TRUE)  

op_bot_rf

bot_rf$confusion
op_bot_rf$confusion

# Optimized model is a lot better; classifying more bots as bots

```

# Optimized Model Visualization
```{r, message=FALSE, echo = FALSE, warning=FALSE}

dev.off()
# varImpPlot(op_bot_rf,
#            sort = TRUE,
#            n.var = 10, 
#            main = "Important Factors for Identifying Twitter Bots",
#            bg = "white",
#            color = "blue",
#            lcolor = "orange")

```

# Optimized Model Prediction
```{r, message=FALSE, echo = FALSE, warning=FALSE}

bot_predict = predict(op_bot_rf,
                            bot_data_test,
                            type = "response",
                            predict.all = TRUE,
                            proximity = TRUE)

```

# Evaluate Model {.tabset}
## Confusion Matrix
```{r, message=FALSE, echo = FALSE, warning=FALSE}

bot_test_pred = data.frame(bot_data_test, 
                                 Prediction = bot_predict$predicted$aggregate)

# Create the confusion matrix.
bot_test_matrix_rf = table(bot_test_pred$bot, 
                            bot_test_pred$Prediction)

bot_test_matrix_rf

confusionMatrix(bot_test_pred$Prediction,bot_test_pred$bot,positive = "1", 
                dnn=c("Prediction", "Actual"), mode = "everything")


```

## Error Rate
```{r, message=FALSE, echo = FALSE, warning=FALSE}

bot_test_error_rate_rf = sum(bot_test_matrix_rf[row(bot_test_matrix_rf) != 
                                                    col(bot_test_matrix_rf)]) / sum(bot_test_matrix_rf)

bot_test_error_rate_rf

```

